# -*- coding: utf-8 -*-
"""Marketing Analytics Final Project Code

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MKHuX_dUqRaUAtJeNixOZYfJRcswEj7W

"""

# UPLOAD the dataset

from google.colab import files
uploaded = files.upload()

# EXTRACT ZIP

import zipfile

with zipfile.ZipFile("dataverse_files.zip", 'r') as zip_ref:
    zip_ref.extractall("data")

# LOAD the CSVs

import pandas as pd

df = pd.read_csv("data/amazon-purchases.csv")
survey = pd.read_csv("data/survey.csv")
fields = pd.read_csv("data/fields.csv")

# VERIFY

df.head()
survey.head()
fields.head()

"""**STEP A: DATA PREPARATION**





"""

import pandas as pd
import numpy as np


# STEP 1: LOAD THE TRANSACTION DATA
# (Assuming files were extracted from dataverse_files.zip into /data)

df = pd.read_csv("data/amazon-purchases.csv")

# STEP 2: RENAME COLUMNS TO CLEAN, STANDARD NAMES

df = df.rename(columns={
    'Order Date': 'purchase_date',
    'Purchase Price Per Unit': 'price',
    'Quantity': 'quantity',
    'ASIN/ISBN (Product Code)': 'product_id',
    'Survey ResponseID': 'customer_id',
    'Shipping Address State': 'state',
    'Title': 'title',
    'Category': 'category'
})


# STEP 3: REMOVE DUPLICATE ROWS

df = df.drop_duplicates()


# STEP 4: REMOVE ROWS MISSING KEY FIELDS
# customer_id, product_id, price, quantity, and purchase_date are essential for analysis.

df = df.dropna(subset=['customer_id', 'product_id', 'price', 'quantity', 'purchase_date'])

# STEP 5: CLEAN DATE COLUMN
# Convert purchase_date to datetime, drop rows that fail conversion

df['purchase_date'] = pd.to_datetime(df['purchase_date'], errors='coerce')
df = df.dropna(subset=['purchase_date'])

# STEP 6: CLEAN QUANTITY COLUMN
# Convert to numeric, drop invalid rows, convert to integer

df['quantity'] = pd.to_numeric(df['quantity'], errors='coerce')
df = df.dropna(subset=['quantity'])
df['quantity'] = df['quantity'].astype(int)


# STEP 7: CREATE MONETARY VALUE (TOTAL AMOUNT)
# total_amount = price * quantity

df['total_amount'] = df['price'] * df['quantity']


# STEP 8: REMOVE INVALID VALUES
# Remove negative or zero price, quantity, or amount

df = df[df['quantity'] > 0]
df = df[df['price'] > 0]
df = df[df['total_amount'] > 0]


# STEP 9: FINAL SANITY CHECKS
# Unique customers, date range, preview

print("Total Rows After Cleaning:", len(df))
print("Unique Customers:", df['customer_id'].nunique())
print("Date Range:", df['purchase_date'].min(), "to", df['purchase_date'].max())
print("Top Categories:\n", df['category'].value_counts().head())


# STEP 10: SAVE CLEANED DATAFRAME
# df_clean will be used for RFM, CLV, Clustering, Whitespace, etc.

df_clean = df.copy()

df_clean.head()

"""**STEP B: CORE ANALYSIS**

**1. RFM ANALYSIS**


"""

# RFM ANALYSIS
# 1. Aggregation to customer level
# 2. Recency, Frequency, Monetary computation
# 3. RFM scores and customer segments
# 4. Optional visuals and segment insights


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Head of df_clean is assumed available from previous step
# df_clean contains purchase_date, product_id, total_amount, customer_id

# 1. Reference date for recency calculation
reference_date = df_clean['purchase_date'].max() + pd.Timedelta(days=1)

# 2. Aggregate to customer level
rfm = df_clean.groupby('customer_id').agg({
    'purchase_date': lambda x: (reference_date - x.max()).days,
    'product_id': 'count',
    'total_amount': 'sum'
}).reset_index()

rfm.columns = ['customer_id', 'recency', 'frequency', 'monetary']

# 3. Remove customers with zero spend (rare, safety check)
rfm = rfm[rfm['monetary'] > 0]

# 4. Create RFM scores using quintiles
rfm['R_score'] = pd.qcut(rfm['recency'], 5, labels=[5,4,3,2,1]).astype(int)
rfm['F_score'] = pd.qcut(rfm['frequency'].rank(method="first"), 5, labels=[1,2,3,4,5]).astype(int)
rfm['M_score'] = pd.qcut(rfm['monetary'], 5, labels=[1,2,3,4,5]).astype(int)

# 5. Combined RFM score
rfm['RFM_score'] = (
    rfm['R_score'].astype(str) +
    rfm['F_score'].astype(str) +
    rfm['M_score'].astype(str)
)

# 6. Customer segment rules
def classify_segment(row):
    if row['R_score'] >= 4 and row['F_score'] >= 4:
        return 'Champions'
    if row['R_score'] >= 4 and row['F_score'] >= 2:
        return 'Loyal'
    if row['R_score'] == 3:
        return 'Potential Loyalist'
    if row['R_score'] == 2:
        return 'Needs Attention'
    if row['R_score'] == 1 and row['F_score'] >= 3:
        return 'At Risk'
    return 'Hibernating'

rfm['segment'] = rfm.apply(classify_segment, axis=1)

print("RFM Summary:")
print(rfm.head())

print("Segment counts:")
print(rfm['segment'].value_counts())



# OPTIONAL VISUALS FOR THE REPORT

plt.figure(figsize=(12,6))
sns.countplot(data=rfm, x='segment', order=rfm['segment'].value_counts().index)
plt.title('Customer Segments by RFM')
plt.xticks(rotation=45)
plt.show()

plt.figure(figsize=(12,6))
sns.histplot(rfm['monetary'], bins=40, kde=True, color='red')
plt.title('Monetary Distribution')
plt.xlabel('Total Spend')
plt.show()

# Grouped bar chart of segment characteristics
segment_stats = rfm.groupby('segment')[['recency', 'frequency', 'monetary']].mean().reset_index()
melted = segment_stats.melt(id_vars='segment', var_name='metric', value_name='value')

plt.figure(figsize=(12,6))
sns.barplot(data=melted, x='segment', y='value', hue='metric')
plt.title('Average RFM Metrics by Segment')
plt.xticks(rotation=45)
plt.show()

# Heatmap of segment characteristics
plt.figure(figsize=(10,6))
sns.heatmap(rfm.groupby('segment')[['recency', 'frequency', 'monetary']].mean(), annot=True, cmap='Blues')
plt.title('Segment-Level RFM Heatmap')
plt.show()


# MANAGERIAL INSIGHTS


insights = {
    'Champions': 'Most loyal and high spending customers. Prioritize with exclusive offers and early access.',
    'Loyal': 'Strong repeat buyers. Encourage with loyalty rewards and personalized recommendations.',
    'Potential Loyalist': 'Growing relationship. Nudge with targeted promotions to convert into loyal customers.',
    'Needs Attention': 'Recently active but low engagement. Re-engage with discounts and reminders.',
    'At Risk': 'High value but inactive recently. Use win-back campaigns, surveys, and special incentives.',
    'Hibernating': 'Dormant customers with low frequency. Use broad promotions or reactivation offers.'
}

insight_df = pd.DataFrame.from_dict(insights, orient='index', columns=['Recommended Action'])
display(insight_df)

"""**2. CLV ANALYSIS**
- BG-NBD: model for predicted purchases
- Gamma-Gamma: model for predicted monetary value
"""

# Compute customer lifetime value using any model
!pip install lifetimes

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from lifetimes import BetaGeoFitter, GammaGammaFitter

# Prepare summary dataset for CLV modeling
clv_df = df_clean.copy()
clv_df = clv_df.sort_values(by=['customer_id', 'purchase_date'])

summary = clv_df.groupby('customer_id').agg({
    'purchase_date': [
        lambda x: (x.max() - x.min()).days,       # Recency
        lambda x: (reference_date - x.min()).days # Customer age T
    ],
    'product_id': 'count',                        # Frequency
    'total_amount': 'mean'                        # Monetary
})

summary.columns = ['recency', 'T', 'frequency', 'monetary']
summary = summary[summary['monetary'] > 0]

# BG-NBD model for frequency prediction
bgf = BetaGeoFitter(penalizer_coef=0.01)
bgf.fit(summary['frequency'], summary['recency'], summary['T'])

# Gamma-Gamma model for monetary prediction
ggf = GammaGammaFitter(penalizer_coef=0.01)
ggf.fit(summary['frequency'], summary['monetary'])



# Clearly explain and justify the lifespan assumption if you use it
# We assume a 6-month prediction horizon (180 days) because:
# 1. The dataset spans many years, showing long-term customer behavior.
# 2. Retail customers purchase infrequently, so 6 months is realistic.
# 3. Longer horizons (12â€“24 months) inflate CLV too much for low-frequency buyers.

summary['CLV'] = ggf.customer_lifetime_value(
    bgf,
    summary['frequency'],
    summary['recency'],
    summary['T'],
    summary['monetary'],
    time=6,              # 6 month lifespan
    discount_rate=0.01
)



# Compare CLV across RFM segments and interpret
summary = summary.merge(rfm[['customer_id', 'segment']], on='customer_id', how='left')

clv_segment = summary.groupby('segment')['CLV'].mean().sort_values(ascending=False)
print("Average CLV by Segment:")
print(clv_segment)

plt.figure(figsize=(12,6))
sns.barplot(x=clv_segment.index, y=clv_segment.values, palette='viridis')
plt.title('Average CLV by RFM Segment')
plt.xlabel('Segment')
plt.ylabel('Average CLV')
plt.xticks(rotation=45)
plt.show()

"""**3. CUSTOMER CLUSTERING**


"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score
from scipy.cluster.hierarchy import linkage, dendrogram, fcluster

# Build clustering dataset using behavioral features
# We use recency, frequency, monetary, CLV
clust_data = summary[['recency', 'frequency', 'monetary', 'CLV']].dropna().copy()

# Keep a reference to customer ids
clust_data['customer_id'] = summary.loc[clust_data.index, 'customer_id'].values

# Feature matrix for clustering
features = clust_data[['recency', 'frequency', 'monetary', 'CLV']]

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(features)


# Perform Hierarchical Clustering on a sample of customers and interpret the dendrogram

# Take a sample of 500 customers for the dendrogram to keep it readable
sample_size = min(500, len(clust_data))
sample = clust_data.sample(n=sample_size, random_state=42)
X_sample_scaled = scaler.transform(sample[['recency', 'frequency', 'monetary', 'CLV']])

# Compute linkage matrix using Ward method
Z = linkage(X_sample_scaled, method='ward')

# Plot dendrogram
plt.figure(figsize=(12, 6))
dendrogram(Z, truncate_mode='level', p=5)
plt.title('Hierarchical Clustering Dendrogram (Sample of Customers)')
plt.xlabel('Customers (sampled)')
plt.ylabel('Distance')
plt.show()

# In the report you will visually inspect this dendrogram and explain how many clusters look reasonable
# Typically you look for a horizontal cut that crosses a limited number of vertical lines


# Run K-Means Clustering and justify the chosen number of clusters

# Try different values of k and compute inertia and silhouette scores
inertias = []
silhouettes = []
K_range = range(2, 9)

for k in K_range:
    kmeans_temp = KMeans(n_clusters=k, random_state=42, n_init=10)
    labels_temp = kmeans_temp.fit_predict(X_scaled)
    inertias.append(kmeans_temp.inertia_)
    silhouettes.append(silhouette_score(X_scaled, labels_temp))

# Plot elbow curve for inertia
plt.figure(figsize=(10, 4))
plt.plot(list(K_range), inertias, marker='o')
plt.title('Elbow Plot for KMeans')
plt.xlabel('Number of clusters k')
plt.ylabel('Inertia')
plt.show()

# Plot silhouette scores
plt.figure(figsize=(10, 4))
plt.plot(list(K_range), silhouettes, marker='o')
plt.title('Silhouette Score by Number of Clusters')
plt.xlabel('Number of clusters k')
plt.ylabel('Silhouette score')
plt.show()

# Choose the k with the highest silhouette score as a data based justification
best_k = K_range[int(np.argmax(silhouettes))]
print("Chosen number of clusters (best silhouette):", best_k)

# Fit final KMeans model using best_k
kmeans = KMeans(n_clusters=best_k, random_state=42, n_init=10)
clust_data['kmeans_cluster'] = kmeans.fit_predict(X_scaled)


# Compare results across both methods and interpret customer clusters

# For comparison, assign hierarchical clusters on the full dataset using the same number of clusters
Z_full = linkage(X_scaled, method='ward')
hier_labels = fcluster(Z_full, t=best_k, criterion='maxclust')
clust_data['hier_cluster'] = hier_labels

# Cross tabulation of KMeans vs Hierarchical clusters
cluster_compare = pd.crosstab(clust_data['kmeans_cluster'], clust_data['hier_cluster'])
print("Comparison of KMeans and Hierarchical clusters:")
print(cluster_compare)

# Attach clusters back to summary and RFM if needed later
summary = summary.merge(
    clust_data[['customer_id', 'kmeans_cluster', 'hier_cluster']],
    on='customer_id',
    how='left'
)

#  VISUALS: Compare results across both methods and interpret customer clusters


plt.figure(figsize=(8,6))
sns.heatmap(cluster_compare, annot=True, fmt='d', cmap='Blues')
plt.title('KMeans vs Hierarchical Clustering Comparison')
plt.xlabel('Hierarchical Cluster')
plt.ylabel('KMeans Cluster')
plt.show()

# Additional analysis and visuals

# 1. Cluster level profiles for recency, frequency, monetary, CLV
cluster_profile = clust_data.groupby('kmeans_cluster')[['recency', 'frequency', 'monetary', 'CLV']].mean()
print("Cluster profiles (KMeans):")
print(cluster_profile)

plt.figure(figsize=(10, 6))
sns.heatmap(cluster_profile, annot=True, cmap='Blues')
plt.title('KMeans Cluster Profiles (RFM plus CLV)')
plt.xlabel('Metric')
plt.ylabel('Cluster')
plt.show()

# 2. Relationship between clusters and RFM segments
if 'segment' in rfm.columns:
    # Merge RFM segment info
    seg_merge = clust_data.merge(rfm[['customer_id', 'segment']], on='customer_id', how='left')
    seg_counts = pd.crosstab(seg_merge['kmeans_cluster'], seg_merge['segment'])
    print("RFM segments within each KMeans cluster:")
    print(seg_counts)

    plt.figure(figsize=(12, 6))
    seg_counts_norm = seg_counts.div(seg_counts.sum(axis=1), axis=0)
    seg_counts_norm.plot(kind='bar', stacked=True, figsize=(12, 6))
    plt.title('Distribution of RFM Segments within KMeans Clusters')
    plt.xlabel('KMeans Cluster')
    plt.ylabel('Proportion')
    plt.xticks(rotation=0)
    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')
    plt.tight_layout()
    plt.show()

"""**4. WHITESPACE ANALYSIS**


"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Identify cross-sell or up-sell opportunities using product-level purchase data

# Category penetration: percentage of customers buying each category
unique_customers = df_clean['customer_id'].nunique()

category_penetration = (
    df_clean.groupby('category')['customer_id']
    .nunique()
    .sort_values(ascending=False)
    .reset_index()
)

category_penetration['penetration_rate'] = (
    category_penetration['customer_id'] / unique_customers * 100
)

print("Category Penetration Table:")
print(category_penetration.head(10))

plt.figure(figsize=(12,6))
sns.barplot(data=category_penetration.head(15), x='category', y='penetration_rate')
plt.xticks(rotation=90)
plt.title('Top Categories by Customer Penetration')
plt.ylabel('Penetration Rate (%)')
plt.xlabel('Category')
plt.show()


# Highlight categories with low penetration or missed opportunities

# Categories in bottom quartile of penetration
low_pen = category_penetration[
    category_penetration['penetration_rate'] < category_penetration['penetration_rate'].quantile(0.25)
]

print("Low Penetration Categories:")
print(low_pen)


# Cross-sell matrix: customers x categories (1 if customer bought the category)

customer_category_matrix = (
    df_clean
    .groupby(['customer_id', 'category'])['product_id']
    .count()
    .unstack(fill_value=0)
)

customer_category_matrix = (customer_category_matrix > 0).astype(int)

# Correlation of categories to identify cross-sell opportunities
category_corr = customer_category_matrix.corr()

plt.figure(figsize=(10,7))
sns.heatmap(category_corr, cmap='Greens')
plt.title('Category Co-Purchase Correlation Heatmap')
plt.show()


# Provide actionable recommendations

# We define whitespace as:
# Categories with low penetration but strong positive correlation with popular categories

popular_cats = category_penetration.head(5)['category'].tolist()

whitespace_reco = []

for cat in low_pen['category']:
    for pop in popular_cats:
        if category_corr.loc[cat, pop] > 0.15:   # moderate correlation threshold
            whitespace_reco.append({
                'Low Pen Category': cat,
                'High Pen Category': pop,
                'Correlation': category_corr.loc[cat, pop],
                'Recommendation': f"Cross-sell {cat} to customers buying {pop}"
            })

whitespace_df = pd.DataFrame(whitespace_reco)
print("Whitespace Opportunities:")
print(whitespace_df)

"""**STEP C: ADDITIONAL ADVANCED ANALYSIS**

**1. Text mining (sentiment, keyword extraction)**
"""

# TEXT MINING: SENTIMENT + KEYWORD EXTRACTION

!pip install textblob wordcloud

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from textblob import TextBlob
from sklearn.feature_extraction.text import CountVectorizer
from wordcloud import WordCloud

# 1. Prepare a single text field from the survey data
# We take all object (text-like) columns and join them into one string per response

text_cols = survey.select_dtypes(include='object').columns
text_df = survey[text_cols].fillna('').copy()

text_df['text'] = text_df.apply(lambda row: ' '.join(row.values.astype(str)), axis=1)

# Basic cleaning: lowercase
text_df['clean_text'] = text_df['text'].str.lower()


# 2. Sentiment analysis (polarity) using TextBlob

def get_sentiment(text):
    return TextBlob(text).sentiment.polarity

text_df['sentiment'] = text_df['clean_text'].apply(get_sentiment)

# Label as positive, neutral, negative
def label_sentiment(x):
    if x > 0.1:
        return 'positive'
    elif x < -0.1:
        return 'negative'
    else:
        return 'neutral'

text_df['sentiment_label'] = text_df['sentiment'].apply(label_sentiment)

# Plot sentiment distribution
plt.figure(figsize=(10,5))
sns.histplot(text_df['sentiment'], bins=40, kde=True, color='purple')
plt.title('Sentiment Polarity Distribution')
plt.xlabel('Polarity')
plt.ylabel('Count')
plt.show()

# Plot sentiment label counts
sent_counts = text_df['sentiment_label'].value_counts()
plt.figure(figsize=(7,5))
sns.barplot(x=sent_counts.index, y=sent_counts.values, palette='coolwarm')
plt.title('Sentiment Categories')
plt.xlabel('Sentiment')
plt.ylabel('Number of Responses')
plt.show()


# 3. Keyword extraction using CountVectorizer

vectorizer = CountVectorizer(stop_words='english', max_features=30)
X = vectorizer.fit_transform(text_df['clean_text'])

keywords = vectorizer.get_feature_names_out()
counts = X.sum(axis=0).A1

keyword_df = pd.DataFrame({'keyword': keywords, 'count': counts}).sort_values('count', ascending=False)

print("Top keywords:")
print(keyword_df.head(20))

# Plot top keywords
top_k = keyword_df.head(20)
plt.figure(figsize=(12,5))
sns.barplot(data=top_k, x='keyword', y='count')
plt.title('Top Keywords in Survey Responses')
plt.xlabel('Keyword')
plt.ylabel('Frequency')
plt.xticks(rotation=45)
plt.show()


# 4. Word cloud from keyword frequencies

freq_dict = {row['keyword']: row['count'] for _, row in keyword_df.iterrows()}

wc = WordCloud(width=1000, height=500, background_color='white').generate_from_frequencies(freq_dict)

plt.figure(figsize=(12,6))
plt.imshow(wc, interpolation='bilinear')
plt.axis('off')
plt.title('Word Cloud of Customer Feedback')
plt.show()

"""**2. Market Basket analysis**

"""

!pip install mlxtend

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
from mlxtend.frequent_patterns import apriori, association_rules
from mlxtend.preprocessing import TransactionEncoder

# 1. Filter df_clean for MBA
mba = df_clean[['customer_id', 'purchase_date', 'category']].copy()
mba = mba.dropna(subset=['category'])
mba['category'] = mba['category'].astype(str)

# 2. Use only TOP 30 categories for readability
top_cats = mba['category'].value_counts().head(30).index
mba = mba[mba['category'].isin(top_cats)]

# 3. Create baskets (customer + date = transaction)
baskets = (
    mba.groupby(['customer_id', 'purchase_date'])['category']
    .apply(list)
    .reset_index()
)

# 4. One-hot encode baskets
te = TransactionEncoder()
te_ary = te.fit(baskets['category']).transform(baskets['category'])
basket_matrix = pd.DataFrame(te_ary, columns=te.columns_)

# 5. Apriori frequent itemsets
itemsets = apriori(basket_matrix, min_support=0.005, use_colnames=True)
itemsets = itemsets.sort_values('support', ascending=False)

print("Frequent Itemsets:")
print(itemsets.head(10))

# 6. Association rules
rules = association_rules(itemsets, metric='lift', min_threshold=1.1)
rules = rules.sort_values('lift', ascending=False)

print("Top Association Rules:")
print(rules.head(10))


# VISUALS

# A. Top 10 frequent itemsets
plt.figure(figsize=(10,6))
sns.barplot(
    data=itemsets.head(10),
    x='support',
    y=itemsets.head(10)['itemsets'].astype(str),
    palette='Blues_r'
)
plt.title('Top 10 Frequent Itemsets')
plt.xlabel('Support')
plt.ylabel('Itemset')
plt.show()

# B. Lift vs Confidence scatter
plt.figure(figsize=(10,6))
sns.scatterplot(
    data=rules,
    x='confidence',
    y='lift',
    size='support',
    sizes=(20,300),
    hue='lift',
    palette='viridis'
)
plt.title('Association Rules: Lift vs Confidence')
plt.xlabel('Confidence')
plt.ylabel('Lift')
plt.show()

# C. Top 10 cross-sell opportunities (clean table)
cross_sell = rules[['antecedents','consequents','support','confidence','lift']].head(10)
print("Top 10 Cross-Sell Opportunities:")
print(cross_sell)

"""**3. Customer Lifecycle curve**



"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt

# Group customers by number of purchases (frequency)
lifecycle_df = (
    df_clean.groupby('customer_id')
    .agg({
        'quantity':'sum',
        'total_amount':'sum',
    })
    .rename(columns={'quantity':'total_items', 'total_amount':'total_spend'})
    .reset_index()
)

# Compute number of transactions per customer
txn_counts = df_clean.groupby('customer_id')['purchase_date'].nunique()
lifecycle_df['transactions'] = lifecycle_df['customer_id'].map(txn_counts)

# Group by transaction count (frequency)
curve = lifecycle_df.groupby('transactions')['total_spend'].mean().reset_index()

# Plot lifecycle curve
plt.figure(figsize=(10,6))
sns.lineplot(data=curve, x='transactions', y='total_spend', marker='o')
plt.title("Customer Lifecycle Curve (Frequency vs Avg Revenue)")
plt.xlabel("Number of Transactions (Frequency)")
plt.ylabel("Average Total Spend")
plt.grid(True)
plt.show()

"""4. **Time-Series Analysis**"""

# TIME SERIES ANALYSIS: MONTHLY REVENUE TREND

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Ensure date format
df_clean['purchase_date'] = pd.to_datetime(df_clean['purchase_date'])

# Extract year-month
df_clean['year_month'] = df_clean['purchase_date'].dt.to_period('M')

# Compute monthly revenue
monthly_revenue = (
    df_clean.groupby('year_month')['total_amount']
    .sum()
    .reset_index()
)

# Convert back to timestamp for plotting
monthly_revenue['year_month'] = monthly_revenue['year_month'].dt.to_timestamp()

# Plot monthly revenue
plt.figure(figsize=(12,6))
sns.lineplot(
    data=monthly_revenue,
    x='year_month',
    y='total_amount',
    marker='o'
)
plt.title("Monthly Revenue Trend")
plt.xlabel("Month")
plt.ylabel("Revenue")
plt.xticks(rotation=45)
plt.grid(True)
plt.show()
